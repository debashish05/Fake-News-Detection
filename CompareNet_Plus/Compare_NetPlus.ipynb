{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5236fbd-f938-4d3c-b951-57bdebdfa837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcb54675-37c6-4e92-9768-9dabf136f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d34bbd6f-220c-4987-98a5-09301473f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import  TransformerConv\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59f8824f-e03f-4504-b200-2415e02992e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printflag = True\n",
    "printflag1 = True\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c74f48d2-8840-4e78-bca8-61597c636629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ecdd6b0-3d3a-48df-8277-72b0a08f413f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 Ti with Max-Q Design'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a015f53a-ce5a-4cb0-b889-95e0d9acecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Layer\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, inputs, adj, global_W = None):\n",
    "        if len(adj._values()) == 0:\n",
    "            zeros = torch.zeros(adj.shape[0], self.out_features, device=inputs.device, dtype=self.weight.dtype)\n",
    "            return zeros\n",
    "\n",
    "        support = torch.spmm(inputs, self.weight)\n",
    "        if global_W is not None:\n",
    "            support = torch.spmm(support, global_W)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SelfAttention(Module):\n",
    "    \"\"\"docstring for SelfAttention\"\"\"\n",
    "    def __init__(self, in_features, idx, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.idx = idx\n",
    "        self.linear = torch.nn.Linear(in_features, hidden_dim)\n",
    "        # self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        self.leakyrelu = F.leaky_relu\n",
    "        self.a = Parameter(torch.FloatTensor(2 * hidden_dim, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.a.size(1))\n",
    "        self.a.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs size:  node_num * 3 * in_features\n",
    "        x = self.linear(inputs).transpose(0, 1)\n",
    "        self.n = x.size()[0]\n",
    "        x = torch.cat([x, torch.stack([x[self.idx]] * self.n, dim=0)], dim=2)\n",
    "        U = torch.matmul(x, self.a).transpose(0, 1)\n",
    "        U = self.leakyrelu(U)\n",
    "        weights = F.softmax(U, dim=1)\n",
    "        outputs = torch.matmul(weights.transpose(1, 2), inputs).squeeze(1) * 3\n",
    "        return outputs, weights\n",
    "    \n",
    "\n",
    "    \n",
    "class TransforerBlock(torch.nn.Module):\n",
    "    def __init__(self, in_features_list, out_features,  bias=True, gamma = 0.1):\n",
    "        super().__init__()\n",
    "        self.ntype = len(in_features_list)\n",
    "        self.in_features_list = in_features_list\n",
    "        self.out_features = out_features\n",
    "        self.weights: nn.ParameterList = nn.ParameterList()\n",
    "        \n",
    "        for i in range(self.ntype):\n",
    "            cache = Parameter(torch.FloatTensor(in_features_list[i], out_features))\n",
    "            nn.init.xavier_normal_(cache.data, gain=1.414)\n",
    "            self.weights.append( cache )\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "            stdv = 1. / math.sqrt(out_features)\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.att_list: nn.ModuleList = nn.ModuleList()\n",
    "        for i in range(self.ntype):\n",
    "            self.att_list.append(Transformer_InfLevel(out_features, out_features))\n",
    "\n",
    "\n",
    "        #self.conv = TransformerConv(in_channels, out_channels // 2, heads=2,\n",
    "        #                            dropout=0.1, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, inputs_list, adj_list, global_W = None):\n",
    "        \n",
    "        print(\"inputs_list: \", inputs_list[0].size(), \"Length \", len(inputs_list))\n",
    "        print(\"adj List: \",len(adj_list) )\n",
    "        print(\"self.ntype: \", self.ntype)\n",
    "        print(\"self.in_features_list: \",  self.in_features_list)\n",
    "        print(\"self.out_features: \", self.out_features)\n",
    "        print(\"self.weights: \", self.weights) \n",
    "        \n",
    "        \n",
    "        h = []\n",
    "        for i in range(self.ntype):\n",
    "            h.append( torch.spmm(inputs_list[i], self.weights[i]) )\n",
    "        if global_W is not None:\n",
    "            for i in range(self.ntype):\n",
    "                h[i] = (torch.spmm(h[i], global_W))\n",
    "        outputs = []\n",
    "        for t1 in range(self.ntype):\n",
    "            x_t1 = []\n",
    "            for t2 in range(self.ntype):\n",
    "                if len(adj_list[t1][t2]._values()) == 0:\n",
    "                    zeros = torch.zeros(adj_list[t1][t2].shape[0], self.out_features, device=self.bias.device, dtype=self.weights[0].dtype)\n",
    "                    x_t1.append( zeros )\n",
    "                    continue\n",
    "                if self.bias is not None:\n",
    "                    x_t1.append( self.att_list[t1](h[t1], h[t2], adj_list[t1][t2]) + self.bias )\n",
    "                else:\n",
    "                    print(\"h_size: \", h.size(), adj.size())\n",
    "                    x_t1.append( self.att_list[t1](h[t1], h[t2], adj_list[t1][t2]) )\n",
    "            outputs.append(x_t1)\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "class Transformer_InfLevel(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Transformer_InfLevel, self).__init__()\n",
    "\n",
    "        self.a1 = nn.Parameter(torch.zeros(size=(out_channels, 1)))\n",
    "        self.a2 = nn.Parameter(torch.zeros(size=(out_channels, 1)))\n",
    "        nn.init.xavier_normal_(self.a1.data, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.a2.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2, )\n",
    "        self.conv = TransformerConv(in_channels, out_channels)#.cuda()\n",
    "        # self.conv.cuda()\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2, adj):\n",
    "\n",
    "        h = input1\n",
    "        g = input2\n",
    "        N = h.size()[0]\n",
    "        M = g.size()[0]\n",
    "        \n",
    "        print(\"h_size: \", h.size())\n",
    "        print(\"g_size: \", g.size())\n",
    "        print(\"adj: \", adj.size())\n",
    "        \n",
    "\n",
    "        e1 = torch.matmul(h, self.a1).repeat(1, 32)\n",
    "        # e2 = torch.matmul(g, self.a2).repeat(1, N).t()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #e1 = torch.matmul(h, self.a1)\n",
    "        #e2 = torch.matmul(g, self.a2)\n",
    "        \n",
    "        print(\"e1_size: \", e1.size())\n",
    "        #print(\"e2_size: \", e2.size())\n",
    "        \n",
    "        #e = e1 + e2    \n",
    "        #e = torch.matmul(e, g)\n",
    "        e = self.leakyrelu(e1)\n",
    "        a = adj.to_dense().nonzero().t().contiguous().long()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #a = a.cuda()\n",
    "        #e = e.cuda()\n",
    "        print(a.is_cuda, e.is_cuda)     \n",
    "        print(\"e_size\", e.size(), \"a_size\", a.size())\n",
    "        \n",
    "        h_prime = self.conv(e, a)\n",
    "\n",
    "            \n",
    "\n",
    "        # print(\"e1_size: \", e1.size())\n",
    "        # print(\"e2_size: \", e2.size())\n",
    "        #print(\"e_size: \", e.size())\n",
    "        #print(\"self.a1_size\" , self.a1.size())\n",
    "        #print(\"self.a2_size\", self.a2.size())\n",
    "        #print(\"adj: \", adj.size())     \n",
    "        print(\"h_prime: \", h_prime.size())\n",
    "        #print(\"self.in_channels \", self.in_channels)\n",
    "        #print(\"self.out_channels \", self.out_channels)\n",
    "            \n",
    "        return h_prime\n",
    "\n",
    "\n",
    "class GraphAttentionConvolution(Module):\n",
    "    def __init__(self, in_features_list, out_features, bias=True, gamma = 0.1):\n",
    "        super(GraphAttentionConvolution, self).__init__()\n",
    "        self.ntype = len(in_features_list)\n",
    "        self.in_features_list = in_features_list\n",
    "        self.out_features = out_features\n",
    "        self.weights: nn.ParameterList = nn.ParameterList()\n",
    "        \n",
    "        \n",
    "        for i in range(self.ntype):\n",
    "            cache = Parameter(torch.FloatTensor(in_features_list[i], out_features))\n",
    "            nn.init.xavier_normal_(cache.data, gain=1.414)\n",
    "            self.weights.append( cache )\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "            stdv = 1. / math.sqrt(out_features)\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.att_list: nn.ModuleList = nn.ModuleList()\n",
    "        for i in range(self.ntype):\n",
    "            self.att_list.append( Attention_InfLevel(out_features, gamma) )\n",
    "\n",
    "\n",
    "    def forward(self, inputs_list, adj_list, global_W = None):\n",
    "        global printflag1\n",
    "               \n",
    "        # print(\"inputs_list: \", inputs_list[0].size(), \"Length \", len(inputs_list))\n",
    "        # print(\"adj List: \",len(adj_list) )\n",
    "        # print(\"self.ntype: \", self.ntype)\n",
    "        # print(\"self.in_features_list: \",  self.in_features_list)\n",
    "        # print(\"self.out_features: \", self.out_features)\n",
    "        # print(\"self.weights: \", self.weights) \n",
    "        \n",
    "        h = []\n",
    "        for i in range(self.ntype):\n",
    "            h.append( torch.spmm(inputs_list[i], self.weights[i]) )\n",
    "        if global_W is not None:\n",
    "            for i in range(self.ntype):\n",
    "                h[i] = ( torch.spmm(h[i], global_W) )\n",
    "        outputs = []\n",
    "        for t1 in range(self.ntype):\n",
    "            x_t1 = []\n",
    "            for t2 in range(self.ntype):\n",
    "                if len(adj_list[t1][t2]._values()) == 0:\n",
    "                    zeros = torch.zeros(adj_list[t1][t2].shape[0], self.out_features, device=self.bias.device, dtype=self.weights[0].dtype)\n",
    "                    x_t1.append( zeros )\n",
    "                    continue\n",
    "                if self.bias is not None:\n",
    "                    x_t1.append( self.att_list[t1](h[t1], h[t2], adj_list[t1][t2]) + self.bias )\n",
    "                else:\n",
    "                    x_t1.append( self.att_list[t1](h[t1], h[t2], adj_list[t1][t2]) )\n",
    "       \n",
    "            outputs.append(x_t1)\n",
    "        return outputs\n",
    "\n",
    "class Attention_InfLevel(nn.Module):\n",
    "    def __init__(self, dim_features, gamma):\n",
    "        super(Attention_InfLevel, self).__init__()\n",
    "\n",
    "        self.dim_features = dim_features\n",
    "        self.a1 = nn.Parameter(torch.zeros(size=(dim_features, 1)))\n",
    "        self.a2 = nn.Parameter(torch.zeros(size=(dim_features, 1)))\n",
    "        nn.init.xavier_normal_(self.a1.data, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.a2.data, gain=1.414)        \n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2, )\n",
    "        self.gamma = gamma\n",
    "\n",
    "    \n",
    "    def forward(self, input1, input2, adj):\n",
    "        global printflag\n",
    "        # adj = adj.coalesce()\n",
    "        h = input1\n",
    "        g = input2\n",
    "        N = h.size()[0]\n",
    "        M = g.size()[0]\n",
    "        \n",
    "       \n",
    "\n",
    "        e1 = torch.matmul(h, self.a1).repeat(1, M)\n",
    "        e2 = torch.matmul(g, self.a2).repeat(1, N).t()\n",
    "        e = e1 + e2  \n",
    "        e = self.leakyrelu(e)\n",
    "        a = adj.to_dense()\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(a > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = torch.mul(attention, a.sum(1).repeat(M, 1).t())\n",
    "        attention = torch.add(attention * self.gamma, a * (1 - self.gamma))\n",
    "        del zero_vec\n",
    "\n",
    "        h_prime = torch.matmul(attention, g)\n",
    "\n",
    "        #print(\"h_size: \", h.size())\n",
    "        #print(\"g_size: \", g.size())\n",
    "        #print(\"e1_size:\", e1.size())\n",
    "        #print(\"e2_size: \", e2.size())\n",
    "        #print(\"e_size: \", e.size())\n",
    "        #print(\"self.a1_size\" , self.a1.size())\n",
    "        #print(\"self.a2_size\", self.a2.size())\n",
    "        #print(\"adj: \", adj.size())     \n",
    "        #print(\"h_prime: \", h_prime.size())\n",
    "        #print(\"self.dim_features \", self.dim_features)\n",
    "    \n",
    "        \n",
    "        return h_prime\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self, hidden_dimension, embedding_dimension):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.out = nn.Linear(hidden_dimension, embedding_dimension)\n",
    "    def forward(self, input):\n",
    "        _, output = self.bert(**input)\n",
    "        out = self.out(output)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Transformer_Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dimension, embedding_dimension):\n",
    "        super(Transformer_Encoder, self).__init__()\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dimension, 2)\n",
    "        # print(\"hidden_dimension: \", hidden_dimension, \", embedding_dimension: \", embedding_dimension, \", encoder_layers: \", encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers, 4)\n",
    "        \n",
    "    def forward(self, input, x):\n",
    "        out = self.encoder(input)\n",
    "        # print(\"out: \", out.size())\n",
    "        out = out.mean(1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LstmEncoder(Module):\n",
    "    def __init__(self, hidden_dimension, embedding_dimension):\n",
    "        super(LstmEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dimension\n",
    "        self.lstm = nn.LSTM(embedding_dimension, hidden_dimension, batch_first=True)\n",
    "\n",
    "    def forward(self, embeds, seq_lens):\n",
    "        print(\"embeds: \", embeds.size(), \"seq_lens\", seq_lens)\n",
    "        _, idx_sort = torch.sort(seq_lens, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        lens = list(seq_lens[idx_sort])\n",
    "        selected_dim = 0\n",
    "        x = embeds.index_select(selected_dim, idx_sort)\n",
    "        rnn_input = nn.utils.rnn.pack_padded_sequence(x, lens, batch_first=True)\n",
    "        rnn_output, (ht, ct) = self.lstm(rnn_input)\n",
    "        ht = ht[-1].index_select(selected_dim, idx_unsort)\n",
    "        print(rnn_input, ht.size())\n",
    "        return ht  # bs * hidden_dim\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.params = params\n",
    "        hidden_dimension = self.params.node_emb_dim // 2\n",
    "        self.w = nn.Linear(self.params.node_emb_dim, hidden_dimension)\n",
    "        self.a = nn.Linear(hidden_dimension, 1)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, X, dim=0, keepdim=True):\n",
    "        '''\n",
    "        :param X:           A tensor with shape:  D * H\n",
    "        :return:            A tensor with shape:  1 * H (dim = 0)\n",
    "        '''\n",
    "        a = self.w(X)\n",
    "        a = self.leakyrelu(a)\n",
    "        a = self.a(a)         # D * 1\n",
    "        a = torch.softmax(a, dim=dim)\n",
    "        return torch.matmul(a.t(), X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21d499fe-d49b-45fb-8e73-0fbb9e101322",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from models.layer import *\n",
    "from torch.nn.parameter import Parameter\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from functools import reduce\n",
    "import pickle as pkl\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class HGAT(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(HGAT, self).__init__()\n",
    "        self.para_init()\n",
    "        self.attention = True\n",
    "        self.lower_attention = True\n",
    "        \n",
    "        \n",
    "        # self.nonlinear = nn.LeakyReLU(0.2)\n",
    "        self.nonlinear = F.relu_\n",
    "        nfeat_list = [params.hidden_dim] * ({0: 1, 1: 2, 2: 2, 3: 3}[params.node_type])\n",
    "        \n",
    "        self.ntype = len(nfeat_list)\n",
    "        nhid = params.node_emb_dim\n",
    "        # print(\"nfeat_list:\", nfeat_list, \" nhid: \" , nhid)   \n",
    "\n",
    "        self.gc2: nn.ModuleList = nn.ModuleList()\n",
    "        if not self.lower_attention:\n",
    "            # print(\"Inside If Not lower_attention------------1\")\n",
    "            self.gc1: nn.ModuleList = nn.ModuleList()\n",
    "            for t in range(self.ntype):\n",
    "                self.gc1.append( GraphConvolution(nfeat_list[t], nhid, bias=False) )\n",
    "                self.bias1 = Parameter( torch.FloatTensor(nhid) )\n",
    "                stdv = 1. / math.sqrt(nhid)\n",
    "                self.bias1.data.uniform_(-stdv, stdv)\n",
    "        else:\n",
    "            # print(\"Executing GraphAttentionConvolution------------1\", nfeat_list, nhid)\n",
    "            self.gc1 = GraphAttentionConvolution(nfeat_list, nhid, gamma=0.1)\n",
    "            # self.gc1 = TransforerBlock(nfeat_list, nhid, gamma=0.1)  \n",
    "            \n",
    "        if self.attention:\n",
    "            \n",
    "            self.at1: nn.ModuleList = nn.ModuleList()\n",
    "            for t in range(self.ntype):\n",
    "                # print(\"Executing GraphAttention------------1\", \" t: \" , t, \" nhid: \" , nhid )\n",
    "                self.at1.append( SelfAttention(nhid, t, nhid // 2) )\n",
    "        self.dropout = nn.Dropout(params.dropout)\n",
    "\n",
    "    def para_init(self):\n",
    "        print(\"Para Init Invoked\")\n",
    "        self.attention = False\n",
    "        self.lower_attention = False\n",
    "\n",
    "    def forward(self, x_list, adj_list, adj_all = None):\n",
    "        # print(\"ntype: \", self.ntype)\n",
    "        \n",
    "        x0 = x_list\n",
    "\n",
    "        if not self.lower_attention:\n",
    "            # print(\"Inside If Not lower_attention------------2\")\n",
    "            x1 = [None for _ in range(self.ntype)]\n",
    "\n",
    "            for t1 in range(self.ntype):\n",
    "                x_t1 = []\n",
    "                for t2 in range(self.ntype):\n",
    "                    idx = t2\n",
    "                    print(\"t1: \", t1, \" t2: \", idx)\n",
    "                    \n",
    "                    x_t1.append( self.gc1[idx](x0[t2], adj_list[t1][t2]) + self.bias1 )\n",
    "                if self.attention:\n",
    "                    x_t1, weights = self.at1[t1]( torch.stack(x_t1, dim=1) )\n",
    "                else:\n",
    "                    x_t1 = reduce(torch.add, x_t1)\n",
    "                x_t1 = self.dropout(self.nonlinear(x_t1))\n",
    "                x1[t1] = x_t1\n",
    "        else:\n",
    "            # print(\"Executing GraphAttentionConvolution------------2\")\n",
    "            x1 = [None for _ in range(self.ntype)]\n",
    "            x1_in = self.gc1(x0, adj_list)\n",
    "            for t1 in range(len(x1_in)):\n",
    "                x_t1 = x1_in[t1]\n",
    "                if self.attention:\n",
    "                    # print(\"Executing GraphAttention------------2\")\n",
    "                    x_t1, weights = self.at1[t1]( torch.stack(x_t1, dim=1) )\n",
    "                else:\n",
    "                    x_t1 = reduce(torch.add, x_t1)\n",
    "                x_t1 = self.dropout(self.nonlinear(x_t1))\n",
    "                x1[t1] = x_t1\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "    def inference(self, x_list, adj_list, adj_all = None):\n",
    "        return self.forward(x_list, adj_list, adj_all)\n",
    "\n",
    "\n",
    "class TextEncoder(Module):\n",
    "    def __init__(self, params):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        \n",
    "        if(params.encoder == 0):\n",
    "            self.lstm = LstmEncoder(params.hidden_dim, params.emb_dim)\n",
    "        if(params.encoder == 1):\n",
    "            self.lstm = Transformer_Encoder(params.hidden_dim, params.emb_dim)\n",
    "\n",
    "    def forward(self, embeds, seq_lens):\n",
    "        return self.lstm(embeds, seq_lens)\n",
    "\n",
    "class EntityEncoder(Module):\n",
    "    def __init__(self, params):\n",
    "        super(EntityEncoder, self).__init__()\n",
    "        \n",
    "        if(params.encoder == 0):\n",
    "            self.lstm = LstmEncoder(params.hidden_dim, params.emb_dim)\n",
    "        if(params.encoder == 1):\n",
    "            self.lstm = Transformer_Encoder(params.hidden_dim, params.emb_dim)\n",
    "\n",
    "        self.gating = GatingMechanism(params)\n",
    "\n",
    "    def forward(self, embeds, seq_lens, Y):\n",
    "        X = self.lstm(embeds, seq_lens)\n",
    "        return self.gating(X, Y)\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Pooling, self).__init__()\n",
    "        self.mode = params.pooling\n",
    "        self.params = params\n",
    "        if self.mode == 'max':\n",
    "            self.pooling = torch.max\n",
    "        elif self.mode == 'sum':\n",
    "            self.pooling = torch.sum\n",
    "        elif self.mode == 'mean':\n",
    "            self.pooling = torch.mean\n",
    "        elif self.mode == 'att':\n",
    "            self.pooling = AttentionPooling(self.params)\n",
    "        else:\n",
    "            raise Exception(\"Unknown pooling mode: {}. (Supported: max, sum, mean, att)\".format(self.mode))\n",
    "\n",
    "    def forward(self, X, sentPerDoc):\n",
    "        '''\n",
    "        :param X:           A tensor with shape:  (D1 + D2 + ... + Dn) * H\n",
    "        :param sentPerDoc:  A tensor with values: [D1, D2, ..., Dn]\n",
    "        :return:            A tensor with shape:  n * H\n",
    "        '''\n",
    "        # weight = [torch.ones((1, i.item()), device=sentPerDoc.device) for i in sentPerDoc]\n",
    "        # weight = block_diag([m.to_sparse() for m in weight]).to_dense()\n",
    "        sentPerDoc = sentPerDoc.cpu().numpy().tolist()\n",
    "        sents = [X[sum(sentPerDoc[: i]): sum(sentPerDoc[: i+1])] for i in range(len(sentPerDoc))]\n",
    "        output = []\n",
    "        for s in sents:\n",
    "            if s.shape[0] == 0:\n",
    "                output.append(torch.zeros((1, s.shape[1]), device=s.device, dtype=X.dtype))\n",
    "            else:\n",
    "                cache = self.pooling(s, dim=0, keepdim=True)\n",
    "                output.append(cache[0] if isinstance(cache, tuple) else cache)\n",
    "        output = torch.cat(output, dim=0)\n",
    "        return output\n",
    "\n",
    "class ConcatTransform(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ConcatTransform, self).__init__()\n",
    "        self.params = params\n",
    "        self.preW = nn.Linear(self.params.hidden_dim, self.params.node_emb_dim)\n",
    "        self.postW = nn.Linear(self.params.node_emb_dim * 2, self.params.node_emb_dim)\n",
    "        self.dropout = nn.Dropout(self.params.dropout, )\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, Y: torch.FloatTensor):\n",
    "        '''\n",
    "        :param X:   shape: (N, node_emb_dim)\n",
    "        :param Y:   shape: (N, hidden_dim)\n",
    "        :return:    shape: (N, node_emb_dim)\n",
    "        '''\n",
    "        Y = self.preW(self.dropout(Y))                # (N, node_emb_dim)\n",
    "        concatVector = torch.cat([X, Y], dim=1)            # (N, 2 * node_emb_dim)\n",
    "        concatVector = self.postW(self.dropout(concatVector))\n",
    "        return concatVector   # (N, node_emb_dim)\n",
    "\n",
    "class MatchingTransform(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(MatchingTransform, self).__init__()\n",
    "        self.params = params\n",
    "        self.SIMPLE = True\n",
    "        self.preW = nn.Linear(self.params.hidden_dim, self.params.node_emb_dim)\n",
    "        self.postW = nn.Linear(self.params.node_emb_dim * (2 if self.SIMPLE else 4), self.params.node_emb_dim)\n",
    "        # self.nonlinear = nn.LeakyReLU(0.2)\n",
    "        self.dropout = nn.Dropout(self.params.dropout, )\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, Y: torch.FloatTensor):\n",
    "        '''\n",
    "        :param X:   shape: (N, node_emb_dim)\n",
    "        :param Y:   shape: (N, hidden_dim)\n",
    "        :return:    shape: (N, node_emb_dim)\n",
    "        '''\n",
    "        Y = self.preW(self.dropout(Y))                # (N, node_emb_dim)\n",
    "        # if self.SIMPLE:     matchingVector = torch.cat([X - Y, X.mul(Y)], dim=1)            # (N, 2 * node_emb_dim)\n",
    "        if self.SIMPLE:     matchingVector = torch.cat([X - Y, X.mul(Y)], dim=1)            # (N, 2 * node_emb_dim)\n",
    "        else:               matchingVector = torch.cat([X, Y, X - Y, X.mul(Y)], dim=1)      # (N, 4 * node_emb_dim)\n",
    "        matchingVector = self.postW(self.dropout(matchingVector))\n",
    "        return matchingVector   # (N, node_emb_dim)\n",
    "\n",
    "class GatingMechanism(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(GatingMechanism, self).__init__()\n",
    "        self.params = params\n",
    "        with open(self.params.entity_tran, 'rb') as f:\n",
    "            transE_embedding = pkl.load(f)\n",
    "        self.enti_tran = nn.Embedding.from_pretrained(torch.from_numpy(transE_embedding).float())\n",
    "        entity_num = transE_embedding.shape[0]\n",
    "\n",
    "\n",
    "        self.gate_theta = Parameter(torch.empty(entity_num, self.params.hidden_dim))\n",
    "        nn.init.xavier_uniform_(self.gate_theta)\n",
    "\n",
    "        # self.dropout = nn.Dropout(self.params.dropout)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, Y: torch.LongTensor):\n",
    "\n",
    "        gate = torch.sigmoid(self.gate_theta[Y])\n",
    "        Y = self.enti_tran(Y)\n",
    "        output = torch.mul(gate, X) + torch.mul(-gate + 1, Y)\n",
    "        return output\n",
    "\n",
    "\n",
    "## Commented below by sumeet\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     from main import parse_arguments\n",
    "#     GatingMechanism(parse_arguments())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "728d2b9a-d904-4ba8-9fb4-ad38093ae76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifier\n",
    "\n",
    "#!/user/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from functools import reduce\n",
    "# from models.model import HGAT, TextEncoder, EntityEncoder, Pooling, MatchingTransform, GatingMechanism\n",
    "import pickle as pkl\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, params, vocab_size, pte=None):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pte = False if pte is None else True\n",
    "\n",
    "        self.text_encoder = TextEncoder(params)\n",
    "        self.enti_encoder = EntityEncoder(params)\n",
    "        # numOfEntity = 100000\n",
    "        # self.enti_encoder = nn.Embedding(numOfEntity, params.hidden_dim)\n",
    "        # nn.init.xavier_uniform_(self.enti_encoder.weight)\n",
    "        self.topi_encoder = nn.Embedding(100, 100)\n",
    "        self.topi_encoder.from_pretrained(torch.eye(100))\n",
    "        self.match_encoder = MatchingTransform(params)\n",
    "        # self.match_encoder = ConcatTransform(params)   # 参数试验用的\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, params.emb_dim)\n",
    "        if pte is None:\n",
    "            nn.init.xavier_uniform_(self.word_embeddings.weight)\n",
    "        else:\n",
    "            self.word_embeddings.weight.data.copy_(torch.from_numpy(pte))\n",
    "        # KB Field\n",
    "\n",
    "        # with open(self.params.entity_tran, 'rb') as f:\n",
    "        #     transE_embedding = pkl.load(f)\n",
    "        # self.enti_tran = nn.Embedding.from_pretrained(torch.from_numpy(transE_embedding))\n",
    "\n",
    "        self.model = HGAT(params)\n",
    "        self.pooling = Pooling(params)\n",
    "        self.classifier_sen = nn.Linear(params.node_emb_dim, params.ntags)\n",
    "        self.classifier_ent = nn.Linear(params.node_emb_dim, params.ntags)\n",
    "\n",
    "        self.dropout = nn.Dropout(params.dropout, )\n",
    "\n",
    "        # entity_num = transE_embedding.shape[0]\n",
    "        # self.gating = GatingMechanism(params) # 这个要放在最后面，尽量少影响随机初始化\n",
    "\n",
    "    # def forward(self, x_list, adj_list, sentPerDoc, entPerDoc=None):\n",
    "    def forward(self, documents, ent_desc, doc_lens, ent_lens, adj_lists, feature_lists, sentPerDoc, entiPerDoc=None):\n",
    "        x_list = []\n",
    "        embeds_docu = self.word_embeddings(documents)   # sents * max_seq_len * emb\n",
    "        d = self.text_encoder(embeds_docu, doc_lens)    # sents * max_seq_len * hidden\n",
    "        d = self.dropout(F.relu_(d))                     # Relu activation and dropout\n",
    "        x_list.append(d)\n",
    "        if self.params.node_type == 3 or self.params.node_type == 2:\n",
    "            embeds_enti = self.word_embeddings(ent_desc)    # sents * max_seq_len * emb\n",
    "            e = self.enti_encoder(embeds_enti, ent_lens, feature_lists[1])    # sents * max_seq_len * hidden\n",
    "            e = self.dropout(F.relu_(e))                     # Relu activation and dropout\n",
    "            x_list.append(e)\n",
    "        if self.params.node_type == 3 or self.params.node_type == 1:\n",
    "            t = self.topi_encoder(feature_lists[-1])         # tops * hidden\n",
    "            x_list.append(t)\n",
    "\n",
    "        X = self.model(x_list, adj_lists)\n",
    "\n",
    "        X_s = self.pooling(X[0], sentPerDoc)   # 选择句子的部分\n",
    "        output = self.classifier_sen(X_s)\n",
    "\n",
    "        if entiPerDoc is not None:\n",
    "            # E_trans = self.enti_tran(feature_lists[1])\n",
    "            E_GCN = X[1]\n",
    "            # E_KB = self.gating(x_list[1], feature_lists[1])\n",
    "            E_KB = x_list[1]\n",
    "            X_e = self.match_encoder(E_GCN, E_KB)  # 选择实体的部分\n",
    "            X_e = self.pooling(X_e, entiPerDoc)\n",
    "            X_e = self.classifier_ent(X_e)\n",
    "            output += X_e\n",
    "        output = F.softmax(output, dim=1)       # 单分类\n",
    "        # output = torch.sigmoid(output)        # 多分类\n",
    "        return output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04f0b3a7-cae6-4e17-be8d-b279c4ac0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluator\n",
    "\n",
    "import torch, json\n",
    "# from models import Classifier\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from util import Utils\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, params, utils, data_loader):\n",
    "        self.params = params\n",
    "        self.utils = utils\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def get_sentences_from_indices(self, docs):\n",
    "        actual_sentences = []\n",
    "        for doc, sent_lens in docs:\n",
    "            sentences = []\n",
    "            for i, sent in enumerate(doc):\n",
    "                sentences.append(' '.join([self.data_loader.i2w[int(wid)] for wid in sent[:sent_lens[i]]]))\n",
    "            actual_sentences.append(sentences)\n",
    "        return actual_sentences\n",
    "\n",
    "    def _evaluate_aux(self, model, data_loader):\n",
    "        hits = 0\n",
    "        total = 0\n",
    "        all_actual = None\n",
    "        all_predicted = None\n",
    "        for inputs in tqdm(data_loader):\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    documents, ent_desc, doc_lens, ent_lens, y_batch, adj_lists, feature_lists, sentPerDoc, entiPerDoc = \\\n",
    "                        [self.utils.to_gpu(i, self.params.cuda and torch.cuda.is_available()) for i in inputs]\n",
    "                    total += sentPerDoc.shape[0]\n",
    "                    logits = model(documents, ent_desc, doc_lens, ent_lens, adj_lists, feature_lists, sentPerDoc, entiPerDoc)\n",
    "                    predicted = torch.argmax(logits, dim=1)\n",
    "                    hits += torch.sum(predicted == y_batch).item()\n",
    "                    all_predicted = predicted.cpu().data.numpy() if all_predicted is None \\\n",
    "                        else np.concatenate((all_predicted, predicted.cpu().data.numpy()))\n",
    "                    labels = y_batch.cpu().numpy()\n",
    "                    all_actual = labels if all_actual is None else np.concatenate((all_actual, labels))\n",
    "                except RuntimeError as e:\n",
    "                    if 'out of memory' in str(e).lower():\n",
    "                        # outOfMemory += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(e)\n",
    "                        exit()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    exit()\n",
    "        accuracy = hits / total\n",
    "        return accuracy, all_actual, all_predicted\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(json.dumps(vars(self.params), indent=2))\n",
    "\n",
    "        model: torch.nn.Module = Classifier(self.params, vocab_size=len(self.data_loader.w2i), pte=None)\n",
    "        if self.utils.HALF:\n",
    "            model.half()\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        # Load the model weights\n",
    "        \n",
    "        current_model_dict = model.state_dict()\n",
    "        loaded_state_dict = torch.load(\"ckpt/\" + params.model_file, map_location=lambda storage, loc: storage)\n",
    "        new_state_dict={k:v if v.size()==current_model_dict[k].size()  else  current_model_dict[k] for k,v in zip(current_model_dict.keys(), loaded_state_dict.values())}\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "\n",
    "        #model.load_state_dict(torch.load(\"ckpt/\" + self.params.model_file, map_location=lambda storage, loc: storage)) \n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # This dataset is only available for the binary classifier\n",
    "        if self.params.ntags == 2:\n",
    "            accuracy, all_actual, all_predicted = self._evaluate_aux(model, self.data_loader.test_data_loader)\n",
    "            prec_mac, recall_mac, f1_mac, _ = precision_recall_fscore_support(all_actual, all_predicted, average='macro')\n",
    "            prec_mic, recall_mic, f1_mic, _ = precision_recall_fscore_support(all_actual, all_predicted, average='micro')\n",
    "            print(\"Accuracy on the OOD test set 1: {:.4f}\".format(accuracy))\n",
    "            print(\"Precision on the OOD test set 1 macro / micro: {:.4f}, {:.4f}\".format(prec_mac, prec_mic))\n",
    "            print(\"Recall on the OOD test set 1 macro / micro: {:.4f}, {:.4f}\".format(recall_mac, recall_mic))\n",
    "            print(\"F1 on the OOD test set 1 macro / micro: {:.4f}, {:.4f}\".format(f1_mac, f1_mic))\n",
    "            print(\"Latex: {:5.2f} & {:5.2f} & {:5.2f} & {:5.2f}\".format(accuracy*100, prec_mac*100, recall_mac*100, f1_mac*100))\n",
    "            print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "        accuracy, all_actual, all_predicted = self._evaluate_aux(model, self.data_loader.test_data_loader_2)\n",
    "        prec_mac, recall_mac, f1_mac, _ = precision_recall_fscore_support(all_actual, all_predicted, average='macro')\n",
    "        prec_mic, recall_mic, f1_mic, _ = precision_recall_fscore_support(all_actual, all_predicted, average='micro')\n",
    "        print(\"Accuracy on the OOD test set 2: {:.4f}\".format(accuracy))\n",
    "        print(\"Precision on the OOD test set 2 macro / micro: {:.4f}, {:.4f}\".format(prec_mac, prec_mic))\n",
    "        print(\"Recall on the OOD test set 2 macro / micro: {:.4f}, {:.4f}\".format(recall_mac, recall_mic))\n",
    "        print(\"F1 on the OOD test set 2 macro / micro: {:.4f}, {:.4f}\".format(f1_mac, f1_mic))\n",
    "        print(\"Latex: {:5.2f} & {:5.2f} & {:5.2f} & {:5.2f}\".format(accuracy * 100, prec_mac * 100, recall_mac * 100, f1_mac * 100))\n",
    "\n",
    "        #ascii if self.params.ntags == 4:\n",
    "        #     results = confusion_matrix(all_actual, all_predicted)\n",
    "        #     df_cm = pd.DataFrame(results, index=[i for i in [\"Satire\", \"Hoax\", \"Propaganda\", \"Trusted\"]],\n",
    "        #                          columns=[i for i in [\"Satire\", \"Hoax\", \"Propaganda\", \"Trusted\"]])\n",
    "        #     sns_plot = sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "        #     plt.yticks(rotation=45)\n",
    "        #     sns_plot.get_figure().savefig('plots/cm.png')\n",
    "        # \n",
    "        # print(\"----------------------------------------------------------------------\")\n",
    "        # accuracy, all_actual, all_predicted = self._evaluate_aux(model, self.data_loader.dev_data_loader)\n",
    "        # prec_mac, recall_mac, f1_mac, _ = precision_recall_fscore_support(all_actual, all_predicted, average='macro')\n",
    "        # prec_mic, recall_mic, f1_mic, _ = precision_recall_fscore_support(all_actual, all_predicted, average='micro')\n",
    "        # print(\"Accuracy on the dev set: {:.4f}\".format(accuracy))\n",
    "        # print(\"Precision on the dev set macro / micro: {:.4f}, {:.4f}\".format(prec_mac, prec_mic))\n",
    "        # print(\"Recall on the dev macro / micro: {:.4f}, {:.4f}\".format(recall_mac, recall_mic))\n",
    "        # print(\"F1 on the dev macro / micro: {:.4f}, {:.4f}\".format(f1_mac, f1_mic))\n",
    "        # print(\"Latex: {:5.2f} & {:5.2f} & {:5.2f} & {:5.2f}\".format(accuracy * 100, prec_mac * 100, recall_mac * 100, f1_mac * 100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a19d4360-af70-4da2-bc94-7e3809031ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Util\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from models import Classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Utils:\n",
    "    def __init__(self, params, dl):\n",
    "        self.params = params\n",
    "        self.data_loader = dl\n",
    "        self.HALF = params.HALF\n",
    "\n",
    "    @staticmethod\n",
    "    def to_half(arr):\n",
    "        if arr is None:\n",
    "            return arr\n",
    "        if isinstance(arr, list) or isinstance(arr, tuple):\n",
    "            return [Utils.to_half(a) for a in arr]\n",
    "        elif isinstance(arr, torch.FloatTensor) or isinstance(arr, torch.sparse.FloatTensor):\n",
    "            return arr.half()\n",
    "        else:\n",
    "            return arr\n",
    "\n",
    "    def to_gpu(self, arr, cuda):\n",
    "        if self.params.HALF:\n",
    "            arr = Utils.to_half(arr)\n",
    "\n",
    "        if not cuda or arr is None:\n",
    "            return arr\n",
    "        if isinstance(arr, list) or isinstance(arr, tuple):\n",
    "            return [self.to_gpu(a, cuda) for a in arr]\n",
    "        else:\n",
    "            try:\n",
    "                return arr.cuda()\n",
    "            except:\n",
    "                return arr\n",
    "        # elif isinstance(arr[0], int) or isinstance(arr[0], float):\n",
    "        #     return arr\n",
    "        # else:\n",
    "        #     raise TypeError(\"Unknown type of input of 'utils.py/Utils/to_gpu': {}.\".format(type(arr)))\n",
    "\n",
    "    def get_dev_loss_and_acc(self, model, loss_fn):\n",
    "        losses = []; hits = 0; total = 0; outOfMemoryCnt = 0\n",
    "        model.eval()\n",
    "        for inputs in tqdm(self.data_loader.dev_data_loader):\n",
    "            # torch.cuda.empty_cache()\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    documents, ent_desc, doc_lens, ent_lens, y_batch, adj_lists, feature_lists, sentPerDoc, entiPerDoc = \\\n",
    "                        [self.to_gpu(i, self.params.cuda and torch.cuda.is_available()) for i in inputs]\n",
    "                    logits = model(documents, ent_desc, doc_lens, ent_lens, adj_lists, feature_lists, sentPerDoc, entiPerDoc)\n",
    "                    loss = loss_fn(logits, y_batch)\n",
    "                    hits += torch.sum(torch.argmax(logits, dim=1) == y_batch).item()\n",
    "                    total += sentPerDoc.shape[0]\n",
    "                    losses.append(loss.item())\n",
    "                except RuntimeError as e:\n",
    "                    if 'out of memory' in str(e).lower():\n",
    "                        # outOfMemory += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(e)\n",
    "                        exit()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    exit()\n",
    "        if outOfMemoryCnt > 0:\n",
    "            print(\"outOfMemoryCnt when validating: \", outOfMemoryCnt)\n",
    "        return np.asscalar(np.mean(losses)), hits / total\n",
    "\n",
    "    def train(self, save_plots_as, pretrained_emb=None):\n",
    "        model: nn.Module = Classifier(self.params, vocab_size=len(self.data_loader.w2i), pte=pretrained_emb)\n",
    "        if self.params.HALF:\n",
    "            model.half()\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        if self.params.cuda:\n",
    "            model = model.cuda()\n",
    "        # optimizer = optim.Adam(model.parameters(), lr=self.params.lr, weight_decay=self.params.weight_decay)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.params.lr, weight_decay=self.params.weight_decay, eps=1e-4)\n",
    "\n",
    "        # Variables for plotting\n",
    "        train_losses, dev_losses, train_accs, dev_accs = [], [], [], []\n",
    "        s_t = timer()\n",
    "        prev_best = 0\n",
    "        patience = 0\n",
    "        outOfMemory = 0\n",
    "        # Start the training loop\n",
    "        for epoch in range(1, self.params.max_epochs + 1):\n",
    "            model.train()\n",
    "            train_loss, hits, total = 0, 0, 0\n",
    "            # train_data_loader = self.data_loader.train_data_loader \\\n",
    "            #                     if not self.params.DEBUG else self.data_loader.test2_data_loader\n",
    "            for inputs in tqdm(self.data_loader.train_data_loader):\n",
    "                # torch.cuda.empty_cache()\n",
    "                try:\n",
    "                    documents, ent_desc, doc_lens, ent_lens, y_batch, adj_lists, feature_lists, sentPerDoc, entiPerDoc = \\\n",
    "                                                        [self.to_gpu(i, self.params.cuda and torch.cuda.is_available()) for i in inputs]\n",
    "                    total += sentPerDoc.shape[0]\n",
    "                    logits = model(documents, ent_desc, doc_lens, ent_lens, adj_lists, feature_lists, sentPerDoc, entiPerDoc)\n",
    "                    if torch.isnan(logits).any():\n",
    "                        print('stop here')\n",
    "                        # model(documents, ent_desc, doc_lens, ent_lens, adj_lists, feature_lists, sentPerDoc, entiPerDoc)\n",
    "                    loss = loss_fn(logits, y_batch)\n",
    "                    # Book keeping\n",
    "                    train_loss += loss.item()\n",
    "                    hits += torch.sum(torch.argmax(logits, dim=1) == y_batch).item()\n",
    "                    # Back-prop\n",
    "                    optimizer.zero_grad()  # Reset the gradients\n",
    "                    loss.backward()  # Back propagate the gradients\n",
    "                    optimizer.step()  # Update the network\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    if 'out of memory' in str(e).lower():\n",
    "                        outOfMemory += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(e)\n",
    "                        exit()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    exit()\n",
    "            print(\"Times of out of memory: \", outOfMemory)\n",
    "            # Compute loss and acc for dev set\n",
    "            dev_loss, dev_acc = self.get_dev_loss_and_acc(model, loss_fn)\n",
    "            train_loss = train_loss / len(self.data_loader.train_data_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            dev_losses.append(dev_loss)\n",
    "            train_accs.append(hits / total)\n",
    "            dev_accs.append(dev_acc)\n",
    "            tqdm.write(\"Epoch: {}, Train loss: {:.4f}, Train acc: {:.4f}, Dev loss: {:.4f}, Dev acc: {:.4f}\".format(\n",
    "                        epoch, train_loss, hits / total, dev_loss, dev_acc))\n",
    "            if dev_acc < prev_best:\n",
    "                patience += 1\n",
    "                if patience == 3:\n",
    "                    # Learning rate annealing\n",
    "                    optim_state = optimizer.state_dict()\n",
    "                    optim_state['param_groups'][0]['lr'] = optim_state['param_groups'][0]['lr'] / 2\n",
    "                    optimizer.load_state_dict(optim_state)\n",
    "                    tqdm.write('Dev accuracy did not increase, reducing the learning rate by 2!!!')\n",
    "                    patience = 0\n",
    "            else:\n",
    "                prev_best = dev_acc\n",
    "                # Save the model\n",
    "                torch.save(model.state_dict(), \"ckpt/model_{}.t7\".format(save_plots_as))\n",
    "\n",
    "        # Acc vs time plot\n",
    "        fig = plt.figure()\n",
    "        plt.plot(range(1, self.params.max_epochs + 1), train_accs, color='b', label='train')\n",
    "        plt.plot(range(1, self.params.max_epochs + 1), dev_accs, color='r', label='dev')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.legend()\n",
    "        plt.xticks(np.arange(1, self.params.max_epochs + 1, step=4))\n",
    "        fig.savefig('result/' + '{}_accuracy.png'.format(save_plots_as))\n",
    "\n",
    "        return timer() - s_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a4179fc-6038-4a20-88cb-3e4de3f88c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Trainer and print_log\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, params, utils):\n",
    "        self.params = params\n",
    "        self.utils = utils\n",
    "        self.log_time = {}\n",
    "\n",
    "    def train(self):\n",
    "        print('-----------{}-------------'.format(self.params.config))\n",
    "        training_time = self.utils.train(save_plots_as=self.params.config)\n",
    "        self.log_time[self.params.config] = training_time\n",
    "        print('-----------------------------------------')\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, filename=\"Default.log\"):\n",
    "        self.terminal = sys.stdout\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        self.log = open(filename, \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "    def change_file(self, filename=\"Default.log\"):\n",
    "        self.log.close()\n",
    "        self.log = open(filename, \"a\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.stdout = Logger(\"yourlogfilename2.txt\")\n",
    "    print('content.')        \n",
    "        \n",
    "        \n",
    "class Trainer:\n",
    "    def __init__(self, params, utils):\n",
    "        self.params = params\n",
    "        self.utils = utils\n",
    "        self.log_time = {}\n",
    "\n",
    "    def train(self):\n",
    "        print('-----------{}-------------'.format(self.params.config))\n",
    "        training_time = self.utils.train(save_plots_as=self.params.config)\n",
    "        self.log_time[self.params.config] = training_time\n",
    "        print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11a6f6a8-061e-4e9a-b091-c0dd0fe321b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Loader\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from multiprocessing import Pool as ProcessPool\n",
    "\n",
    "ASYMMETRIC = True\n",
    "DEBUG_NUM = 400\n",
    "W2I = None\n",
    "\n",
    "\n",
    "def sentence_tokenize(doc):\n",
    "    # return doc.split('.')\n",
    "    return sent_tokenize(doc)\n",
    "\n",
    "def read_and_unpkl(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pkl.load(f)\n",
    "\n",
    "def parseLine(args):\n",
    "    idx, tag, doc = args\n",
    "    global W2I\n",
    "    # sentences = doc.split('.')\n",
    "    sentences = sentence_tokenize(doc)\n",
    "    sentences_idx = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower().strip().split(\" \")\n",
    "        curr_sentence_idx = [W2I[x] for x in sentence]\n",
    "        sentences_idx.append(curr_sentence_idx if len(curr_sentence_idx) > 0 else [W2I['<unk>']])\n",
    "    return int(tag), sentences_idx\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.ntags = params.ntags\n",
    "\n",
    "        train_pkl_path = '{}/train/'.format(params.adjs)\n",
    "        test_pkl_path = '{}/test/'.format(params.adjs)\n",
    "        dev_pkl_path = '{}/dev/'.format(params.adjs)\n",
    "        print('Loading adj: ', train_pkl_path[: -6])\n",
    "\n",
    "        w2i_pkl_path = params.root + 'w2i.pkl'\n",
    "        if params.mode == 0:\n",
    "            w2i = freezable_defaultdict(lambda: len(w2i))\n",
    "            UNK = w2i[\"<unk>\"]\n",
    "\n",
    "            self.train, self.adj_train, self.fea_train = self.read_dataset(params.train, w2i, train_pkl_path)\n",
    "            print(\"Average train document length: {}\".format(np.mean([len(x[0]) for x in self.train])))\n",
    "            print(\"Maximum train document length: {}\".format(max([len(x[0]) for x in self.train])))\n",
    "\n",
    "            self.train, self.dev, self.adj_train, self.adj_dev, self.fea_train, self.fea_dev = \\\n",
    "                train_test_split(self.train, self.adj_train, self.fea_train, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            w2i = freezable_defaultdict(lambda: len(w2i))\n",
    "            UNK = w2i[\"<unk>\"]\n",
    "\n",
    "            self.train, self.adj_train, self.fea_train = self.read_dataset(params.train, w2i, train_pkl_path)\n",
    "            print(\"Average train document length: {}\".format(np.mean([len(x[0]) for x in self.train])))\n",
    "            print(\"Maximum train document length: {}\".format(max([len(x[0]) for x in self.train])))\n",
    "\n",
    "        w2i = freezable_defaultdict(lambda: UNK, w2i)\n",
    "        w2i.freeze()\n",
    "        self.w2i = w2i\n",
    "        self.i2w = dict(map(reversed, self.w2i.items()))\n",
    "        self.nwords = len(w2i)\n",
    "\n",
    "\n",
    "        with open(params.entity_desc, 'rb') as f:\n",
    "            corpus = pkl.load(f)\n",
    "        self.entity_description = []\n",
    "        for row in corpus:\n",
    "            self.entity_description.append([w2i[x] for x in row.lower().split(\" \")])\n",
    "\n",
    "        if params.mode == 0:\n",
    "            dataset_train = DataSet(self.train, self.adj_train, self.fea_train, self.params, self.entity_description)\n",
    "            self.train_data_loader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                    batch_size=params.batch_size, collate_fn=dataset_train.collate, shuffle=True)\n",
    "            dataset_dev = DataSet(self.dev, self.adj_dev, self.fea_dev, self.params, self.entity_description)\n",
    "            self.dev_data_loader = torch.utils.data.DataLoader(dataset_dev,\n",
    "                                    batch_size=params.batch_size, collate_fn=dataset_dev.collate,   shuffle=False)\n",
    "\n",
    "\n",
    "        self.test, self.adj_test, self.fea_test = self.read_dataset(params.test, w2i, test_pkl_path)\n",
    "        self.test_2, self.adj_test_2, self.fea_test_2 = self.read_dataset(params.dev, w2i, dev_pkl_path)\n",
    "\n",
    "        dataset_test = DataSet(self.test, self.adj_test, self.fea_test, self.params, self.entity_description)\n",
    "        self.test_data_loader = torch.utils.data.DataLoader(dataset_test,\n",
    "                                batch_size=params.batch_size, collate_fn=dataset_test.collate,  shuffle=False)\n",
    "        dataset_test_2 = DataSet(self.test_2, self.adj_test_2, self.fea_test_2, self.params, self.entity_description)\n",
    "        self.test_data_loader_2 = torch.utils.data.DataLoader(dataset_test_2,\n",
    "                                batch_size=params.batch_size, collate_fn=dataset_test_2.collate,shuffle=False)\n",
    "\n",
    "\n",
    "    def load_adj_and_other(self, path):\n",
    "        print(\"Loading {}\".format(path))\n",
    "        if path[-1] == '/':\n",
    "            files = sorted([path + f for f in os.listdir(path) if judge_data(f)],\n",
    "                                key=lambda x: int(x.split('/')[-1].split('.')[0]))  # 用idx.pkl中的idx排序\n",
    "            files = files[: DEBUG_NUM] if self.params.DEBUG else files\n",
    "            data = [read_and_unpkl(file) for file in tqdm(files)]\n",
    "        else:\n",
    "            with open(path, 'rb') as f:\n",
    "                data = pkl.load(f)\n",
    "        print(\"Preprocessing {}\".format(path))\n",
    "        res, device = [], 'cuda' if self.params.cuda else 'cpu'\n",
    "        for piece in tqdm(data):\n",
    "            d_idx = piece['idx']\n",
    "            adj_list = [build_spr_coo(a) for a in piece['adj_list']]\n",
    "            feature_list = [piece['s2i'], piece['e2i'], piece['t2i']]\n",
    "            res.append([adj_list, feature_list])\n",
    "        return res\n",
    "\n",
    "    def read_dataset(self, filename, w2i, adj_file):\n",
    "        adj = self.load_adj_and_other(adj_file)\n",
    "        if 'csv' in filename:\n",
    "            return self.read_dataset_sentence_wise(filename, w2i, adj)\n",
    "        if 'xlsx' in filename:\n",
    "            return self.read_testset_sentence_wise(filename, w2i, adj)\n",
    "\n",
    "    def read_dataset_sentence_wise(self, filename, w2i, adj):\n",
    "        data, new_adj, new_fea, removed_idx = [], [], [], []\n",
    "        global W2I\n",
    "        W2I = w2i\n",
    "        # count = 0\n",
    "        adj, fea = zip(*adj)\n",
    "        with open(filename, \"r\") as f:\n",
    "            readCSV = csv.reader(f, delimiter=',')\n",
    "            csv.field_size_limit(100000000)\n",
    "            sents = []\n",
    "            for idx, (tag, doc) in tqdm(enumerate(readCSV)):\n",
    "                if self.params.DEBUG and idx >= DEBUG_NUM:\n",
    "                    break\n",
    "                sents.append([idx, tag, doc])\n",
    "\n",
    "            sentences_idx_list = []\n",
    "            p = ProcessPool(10)\n",
    "            with tqdm(total=len(sents)) as pbar:\n",
    "                for out in p.imap(parseLine, sents):\n",
    "                    sentences_idx_list.append(out)\n",
    "                    pbar.update(1)\n",
    "            p.close()\n",
    "            p.join()\n",
    "\n",
    "            print(len(sentences_idx_list))\n",
    "            allowed_tags = [1, 4] if self.ntags == 2 else [1, 2, 3, 4]\n",
    "            for idx, (tag, sentences_idx) in enumerate(sentences_idx_list):\n",
    "                if tag in allowed_tags:\n",
    "                    if self.ntags == 2:\n",
    "                        tag = tag - 1 if tag == 1 else tag - 3   # Adjust the tag to {0: Satire, 1: Trusted}\n",
    "                    else:\n",
    "                        tag -= 1                                 # {0: Satire, 1: Hoax, 2: Propaganda, 3: Trusted}\n",
    "                    if len(sentences_idx) > 1:\n",
    "                        data.append((sentences_idx[:self.params.max_sents_in_a_doc], tag))\n",
    "                        new_adj.append(adj[idx])\n",
    "                        new_fea.append(fea[idx])\n",
    "                    else:\n",
    "                        removed_idx.append(idx)\n",
    "        print('removed_idx of {}: {}'.format(filename, len(removed_idx)))\n",
    "        print(len(data), len(new_adj))\n",
    "        return data, new_adj, new_fea\n",
    "\n",
    "    def read_dataset_sentence_wise(self, filename, w2i, adj):\n",
    "        data, new_adj, new_fea = [], [], []\n",
    "        # count = 0\n",
    "        adj, fea = zip(*adj)\n",
    "        with open(filename, \"r\") as f:\n",
    "            readCSV = csv.reader(f, delimiter=',')\n",
    "            csv.field_size_limit(100000000)\n",
    "            removed_idx = []\n",
    "            for idx, (tag, doc) in tqdm(enumerate(readCSV)):\n",
    "                if self.params.DEBUG and idx >= DEBUG_NUM:\n",
    "                    break\n",
    "                # sentences = doc.split('.')\n",
    "                sentences = sentence_tokenize(doc)\n",
    "                tag = int(tag)\n",
    "                allowed_tags = [1, 4] if self.ntags == 2 else [1, 2, 3, 4]\n",
    "                if tag in allowed_tags:\n",
    "                    if self.ntags == 2:\n",
    "                        tag = tag - 1 if tag == 1 else tag - 3   # Adjust the tag to {0: Satire, 1: Trusted}\n",
    "                    else:\n",
    "                        tag -= 1                                 # {0: Satire, 1: Hoax, 2: Propaganda, 3: Trusted}\n",
    "                    sentences_idx = []\n",
    "                    for sentence in sentences:\n",
    "                        sentence = sentence.lower().strip().split(\" \")\n",
    "                        curr_sentence_idx = [w2i[x] for x in sentence]\n",
    "                        sentences_idx.append(curr_sentence_idx if len(curr_sentence_idx) > 0 else [w2i['<unk>']])\n",
    "\n",
    "                    if len(sentences_idx) > 1 and len(sentences_idx) < 1000:\n",
    "                        data.append((sentences_idx[:self.params.max_sents_in_a_doc], tag))\n",
    "                        new_adj.append(adj[idx])\n",
    "                        new_fea.append(fea[idx])\n",
    "                    else:\n",
    "                        removed_idx.append(idx)\n",
    "        print('removed_idx of {}: {}'.format(filename, len(removed_idx)))\n",
    "        return data, new_adj, new_fea\n",
    "\n",
    "    def read_testset_sentence_wise(self, filename, w2i, adj):\n",
    "        df = pd.read_excel(filename)\n",
    "        data, new_adj, new_fea = [], [], []\n",
    "        count = 0\n",
    "        adj, fea = zip(*adj)\n",
    "        removed_idx = []\n",
    "        for idx, row in tqdm(enumerate(df.values)):\n",
    "            if self.params.DEBUG and idx >= DEBUG_NUM:\n",
    "                break\n",
    "            # sentences = row[2].split('.')\n",
    "            sentences = sentence_tokenize(row[2])\n",
    "            tag = int(row[0])\n",
    "            # Tag id is reversed in this dataset\n",
    "            tag = tag + 1 if tag == 0 else tag - 1\n",
    "            sentences_idx = []\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.lower().replace(\"\\n\", \" \").strip().split(\" \")\n",
    "                curr_sentence_idx = [w2i[x] for x in sentence]\n",
    "                sentences_idx.append(curr_sentence_idx if len(curr_sentence_idx) > 0 else [w2i['<unk>']])\n",
    "            if len(sentences_idx) > 1:\n",
    "                data.append((sentences_idx, tag))\n",
    "                new_adj.append(adj[count])\n",
    "                new_fea.append(fea[count])\n",
    "            else:\n",
    "                removed_idx.append(idx)\n",
    "            count += 1\n",
    "\n",
    "        print('removed_idx of {}: {}'.format(filename, removed_idx))\n",
    "        return data, new_adj, new_fea\n",
    "\n",
    "def judge_data(fileName):\n",
    "    key = fileName.split('.')[0]\n",
    "    try:\n",
    "        x = int(key)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def build_spr_coo(spr, device='cpu'):\n",
    "    # {'indices': spr.indices(), 'value': spr.values(), 'size': spr.size()}\n",
    "    if not isinstance(spr, dict):\n",
    "        raise TypeError(\"Not recognized type of sparse matrix source: {}\".format(type(spr)))\n",
    "    tensor = torch.sparse.FloatTensor(spr['indices'], spr['value'], spr['size']).coalesce()\n",
    "    return tensor if device == 'cpu' else tensor.to(device)\n",
    "\n",
    "class DataSet(torch.utils.data.TensorDataset):\n",
    "    def __init__(self, data, adj, fea, params, entity_description):\n",
    "        super(DataSet, self).__init__()\n",
    "        self.params = params\n",
    "        # data is a list of tuples (sent, label)\n",
    "        self.sents = [x[0] for x in data]\n",
    "        self.labels = [x[1] for x in data]\n",
    "        self.adjs = adj\n",
    "        self.feas = fea\n",
    "        self.entity_description = entity_description\n",
    "        self.num_of_samples = len(self.sents)\n",
    "        for i, a in enumerate(self.adjs):\n",
    "            assert a[0].shape[0] == len(self.sents[i]),\\\n",
    "                \"dim of adj does not match the num of sent, where the idx is {}\".format(i)\n",
    "            assert a[4].shape[0] == len(self.feas[i][1]), \\\n",
    "                \"dim of adj does not match the num of entity, where the idx is {}\".format(i)\n",
    "            assert a[7].shape[0] == len(self.feas[i][2]), \\\n",
    "                \"dim of adj does not match the num of topic, where the idx is {}\".format(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_of_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sents[idx], len(self.sents[idx]), self.labels[idx], self.adjs[idx], self.feas[idx]\n",
    "\n",
    "    def collate(self, batch):\n",
    "        sents, doc_lens_o, labels, adjs, feas = zip(*batch)\n",
    "        # concatenate & padding\n",
    "        doc_lens, curr_sents = [], []\n",
    "        for doc in sents:\n",
    "            doc_lens += [min(self.params.max_sent_len, len(x)) for x in doc]\n",
    "            curr_sents += doc\n",
    "        padded_sents = np.zeros((len(curr_sents), max(doc_lens)))\n",
    "        for i, sen in enumerate(curr_sents):\n",
    "            padded_sents[i, :len(sen)] = sen[:doc_lens[i]]\n",
    "        documents = torch.from_numpy(padded_sents).long()\n",
    "\n",
    "        new_feas, new_adjs = [], []\n",
    "        fea_doc, fea_ent, fea_top = zip(*feas)\n",
    "        for f in [fea_doc, fea_ent, fea_top]:\n",
    "            fea = torch.from_numpy(np.array(sum([list(i.values()) for i in f], [])))\n",
    "            new_feas.append(fea.long())\n",
    "        for a in zip(*adjs):\n",
    "            new_adjs.append(block_diag(a).float())\n",
    "\n",
    "        labels = torch.from_numpy(np.array(labels)).long()\n",
    "        sentPerDoc = torch.from_numpy(np.array([len(fea[0]) for fea in feas])).int()\n",
    "        entiPerDoc = torch.from_numpy(np.array([len(fea[1]) for fea in feas])).int()\n",
    "        topiPerDoc = torch.from_numpy(np.array([len(fea[2]) for fea in feas])).int()\n",
    "\n",
    "        # concatenate & padding\n",
    "        ent_lens, curr_sents = [], []\n",
    "        for doc in fea_ent:\n",
    "            doc = [self.entity_description[doc[idx]] for idx in range(len(doc))]\n",
    "            ent_lens += [min(self.params.max_sent_len, len(x)) for x in doc]\n",
    "            curr_sents += doc\n",
    "        padded_sents = np.zeros((len(curr_sents), max(ent_lens)))\n",
    "        for i, sen in enumerate(curr_sents):\n",
    "            padded_sents[i, :len(sen)] = sen[:ent_lens[i]]\n",
    "        ent_desc = torch.from_numpy(padded_sents).long()\n",
    "\n",
    "        doc_lens = torch.from_numpy(np.array(doc_lens)).int()\n",
    "        ent_lens = torch.from_numpy(np.array(ent_lens)).int()\n",
    "\n",
    "        if self.params.node_type == 3:\n",
    "            new_adjs = [new_adjs[0:3], new_adjs[3:6], new_adjs[6:9]]\n",
    "            new_adjs[0][1].zero_()    # (√)text -> entity   (X)entity -> text\n",
    "        elif self.params.node_type == 2:    # Document&Entiy\n",
    "            new_adjs = [new_adjs[0:2], new_adjs[3:5]]\n",
    "            new_feas = new_feas[0: 2]\n",
    "            new_adjs[0][1].zero_()\n",
    "        elif self.params.node_type == 1:    # Document&Topic\n",
    "            new_adjs = [[new_adjs[0], new_adjs[2]], [new_adjs[6], new_adjs[8]]]\n",
    "            new_feas = [new_feas[0], new_feas[2]]\n",
    "            ent_desc, ent_lens, entiPerDoc = None, None, None\n",
    "        elif self.params.node_type == 0:\n",
    "            new_adjs = [[new_adjs[0]]]\n",
    "            new_feas = [new_feas[0]]\n",
    "            ent_desc, ent_lens, entiPerDoc = None, None, None\n",
    "        else:\n",
    "            raise Exception(\"Unknown node_type.\")\n",
    "        return documents, ent_desc, doc_lens, ent_lens, labels, new_adjs, new_feas, sentPerDoc, entiPerDoc\n",
    "\n",
    "\n",
    "def block_diag(mat_list: list or tuple):\n",
    "    shape_list = [m.shape for m in mat_list]\n",
    "    bias = torch.LongTensor([0, 0])\n",
    "    indices, values = [], []\n",
    "    for m in mat_list:\n",
    "        indices.append(m.indices() + bias.unsqueeze(1))\n",
    "        values.append(m.values())\n",
    "        bias += torch.LongTensor(list(m.shape))\n",
    "    indices = torch.cat(indices, dim=1)\n",
    "    values = torch.cat(values, dim=0)\n",
    "    res = torch.sparse.FloatTensor(indices, values, size=torch.Size(bias))\n",
    "    return res\n",
    "\n",
    "class freezable_defaultdict(dict):\n",
    "    def __init__(self, default_factory, *args, **kwargs):\n",
    "        self.frozen = False\n",
    "        self.default_factory = default_factory\n",
    "        super(freezable_defaultdict, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        if self.frozen:\n",
    "            return self.default_factory()\n",
    "        else:\n",
    "            self[key] = value = self.default_factory()\n",
    "            return value\n",
    "\n",
    "    def freeze(self):\n",
    "        self.frozen = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "658497c5-0c8d-4561-8b38-1023ea04f702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## main\n",
    "\n",
    "import os, sys, json, torch\n",
    "import argparse, datetime, time\n",
    "import random, numpy as np\n",
    "# from util import Utils\n",
    "# from data_loader import DataLoader\n",
    "#from trainer import Trainer\n",
    "#from evaluator import Evaluator\n",
    "from timeit import default_timer as timer\n",
    "# from print_log import Logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "node_type:\n",
    "    '3 represents three types: Document&Entity&Topic; \\n'\n",
    "    '2 represents two types: Document&Entiy; \\n'\n",
    "    '1 represents two types: Document&Topic; \\n'\n",
    "    '0 represents only one type: Document. '\n",
    "'''\n",
    "# CUDA_VISIBLE_DEVICES_DICT = {0: '4',    1: '3',     2: '4',     3: '5'}\n",
    "# MEMORY_DICT =               {0: 4000,   1: 9500,    2: 7600,    3: 8000}\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Argument parser for Fake News Detection')\n",
    "    data_root_path = 'data/fakeNews/'\n",
    "    parser.add_argument(\"--root\", type=str, default=data_root_path)\n",
    "    \n",
    "    parser.add_argument(\"--train\", type=str, default=data_root_path + 'fulltrain.csv')\n",
    "    parser.add_argument(\"--dev\", type=str, default=data_root_path + 'balancedtest.csv')\n",
    "    parser.add_argument(\"--test\", type=str, default=data_root_path + 'test.xlsx',\n",
    "                        help='Out of domain test set')\n",
    "    parser.add_argument(\"--pte\", type=str, default='', help='Pre-trained embeds')\n",
    "    parser.add_argument(\"--entity_desc\", type=str, help='entity description path.',\n",
    "                        default=data_root_path + 'entityDescCorpus.pkl')\n",
    "    parser.add_argument(\"--entity_tran\", type=str, help='entity transE embedding path.',\n",
    "                        default=data_root_path + 'entity_feature_transE.pkl')\n",
    "    parser.add_argument(\"--adjs\", type=str, default=data_root_path + 'adjs/')\n",
    "    # Hyper-parameters\n",
    "    parser.add_argument(\"--emb_dim\", type=int, default=100)\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=100)\n",
    "    parser.add_argument(\"--node_emb_dim\", type=int, default=32)\n",
    "    parser.add_argument(\"--max_epochs\", type=int, default=5)\n",
    "    parser.add_argument(\"--max_sent_len\", type=int, default=50)\n",
    "    parser.add_argument(\"--max_sents_in_a_doc\", type=int, default=10000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--ntags\", type=int, default=4)         # 4 or 2\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-6)\n",
    "    parser.add_argument(\"--pooling\", type=str, default='max',\n",
    "                        help='Pooling type: \"max\", \"mean\", \"sum\", \"att\". ')\n",
    "\n",
    "    # parser.add_argument(\"--config\", type=str, default='config_default',\n",
    "    #                     help='Name for saving plots')\n",
    "    parser.add_argument(\"--model_file\", type=str, default='model_CompareNet_Max_DET_1112_1338.t7',\n",
    "                        help='For evaluating a saved model')\n",
    "    parser.add_argument(\"--plot\", type=int, default=0, help='set to plot attn')\n",
    "    parser.add_argument(\"--mode\", type=int, default=0, help='0: train&test, 1:test')\n",
    "    # parser.add_argument(\"--cuda\", type=bool, default=True, help='use gpu to speed up or not')\n",
    "    parser.add_argument(\"--cuda\", type=bool, default=True, help='use gpu to speed up or not')\n",
    "    parser.add_argument(\"--device\", type=int, default=0, help='GPU ID. ')\n",
    "    parser.add_argument(\"--HALF\", type=bool, default=True, help='Use half tensor to save memory')\n",
    "\n",
    "    parser.add_argument(\"--DEBUG\", action='store_true', default=False, help='')\n",
    "    parser.add_argument(\"--node_type\", type=int, default=3,\n",
    "                        help='3 represents three types: Document&Entity&Topic; \\n'\n",
    "                             '2 represents two types: Document&Entiy; \\n'\n",
    "                             '1 represents two types: Document&Topic; \\n'\n",
    "                             '0 represents only one type: Document. ')\n",
    "    parser.add_argument('-r', \"--repeat\", type=int, default=1, help='')\n",
    "    parser.add_argument('-s', \"--seed\", type=list, default=[5], help='')\n",
    "    parser.add_argument('-s', \"--encoder\", type=int, default=[5], help='0: LSTM encoder for text,'\n",
    "                                                                         '1: Transformer encoder for text')\n",
    "\n",
    "    for dir in [\"models/\", \"ckpt/\", \"plots/\", \"result/\", \"log/\"]:\n",
    "        if not os.path.exists(dir):   os.makedirs(dir)\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    TIMENOW = (datetime.datetime.utcnow() + datetime.timedelta(hours=8)).strftime(\"%m%d_%H%M\")\n",
    "    NODETYPE = {0: \"D\", 1: \"DT\", 2: \"DE\", 3: \"DET\"}[args.node_type]\n",
    "    if args.mode == 0:\n",
    "        MODELNAME = 'CompareNet_{}_{}_{}'.format(args.pooling.capitalize(), NODETYPE, TIMENOW)\n",
    "        args.model_file = 'model_{}.t7'.format(MODELNAME)\n",
    "        args.config = MODELNAME\n",
    "        sys.stdout = Logger(\"./log/{}_{}.log\".format(MODELNAME, TIMENOW))\n",
    "    else:\n",
    "        MODELNAME = args.model_file.split(\".\")[0]\n",
    "        args.config = MODELNAME\n",
    "        sys.stdout = Logger(\"./log/{}_{}.log\".format(MODELNAME, TIMENOW))\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device)\n",
    "    args.cuda = args.cuda and torch.cuda.is_available()\n",
    "    args.repeat = len(args.seed) if isinstance(args.seed, list) else args.repeat\n",
    "    print(\"TimeStamp: {}\\n\".format(TIMENOW), json.dumps(vars(args), indent=2))\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94e293e3-5ed6-4087-b1b2-860d8fdaf960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1130/1130 [24:18<00:00,  1.29s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 283/283 [02:42<00:00,  1.74it/s]\n",
      "C:\\Users\\sumeet\\AppData\\Local\\Temp\\ipykernel_6840\\2414961396.py:76: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  return np.asscalar(np.mean(losses)), hits / total\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1130/1130 [26:51<00:00,  1.43s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 283/283 [03:39<00:00,  1.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1130/1130 [30:53<00:00,  1.64s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 283/283 [03:00<00:00,  1.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1130/1130 [25:54<00:00,  1.38s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 283/283 [02:57<00:00,  1.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1130/1130 [25:36<00:00,  1.36s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 283/283 [05:40<00:00,  1.20s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 93/93 [00:51<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "def main(params = None):\n",
    "    global dl, test\n",
    "    if params is None:\n",
    "        params = parse_arguments()\n",
    "        \n",
    "    SEED = params.seed\n",
    "    t0 = time.time()\n",
    "    s_t = timer()\n",
    "    #dl = DataLoader(params)\n",
    "\n",
    "    u = Utils(params, dl)\n",
    "    timeDelta = int(time.time()-t0)\n",
    "    print(\"PreCost:\", datetime.timedelta(seconds=timeDelta))\n",
    "    for repeat in range(params.repeat):\n",
    "        print(\"\\n\\n\\n{0} Repeat: {1} {0}\".format('-'*27, repeat))\n",
    "        set_seed( SEED[repeat] if isinstance(SEED, list) else SEED )\n",
    "        print(\"\\n\\n\\n{0}  Seed: {1}  {0}\".format('-'*27, SEED[repeat]))\n",
    "        if params.mode == 0:\n",
    "            # Start training\n",
    "            trainer = Trainer(params, u)\n",
    "            trainer.log_time['data_loading'] = timer() - s_t\n",
    "            trainer.train()\n",
    "            print(trainer.log_time)\n",
    "            print(\"Total time taken (in seconds): {}\".format(timer() - s_t))\n",
    "\n",
    "            evaluator = Evaluator(params, u, dl)\n",
    "            evaluator.evaluate()\n",
    "        elif params.mode == 1:\n",
    "            # Evaluate on the test set\n",
    "            evaluator = Evaluator(params, u, dl)\n",
    "            evaluator.evaluate()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown mode: {}\".format(params.mode))\n",
    "            \n",
    "\n",
    "def set_seed(seed=9699):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    params = parse_arguments()\n",
    "    set_seed(0)\n",
    "    main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe45c51-013f-404b-ac4a-48dc5ff2aff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Optimization-Python] *",
   "language": "python",
   "name": "conda-env-Optimization-Python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
